{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Twitter import Sanders's tweets and save it to tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from twitter import *\n",
    "import json\n",
    "\n",
    "consumer_key = \"vAm5pdHyfy1xbTdwGtTP1A\"\n",
    "consumer_secret = \"7Kp9IIvYbseJzpknZPOXjcXQHfXsSG4hLUFZ9iz1Ws\"\n",
    "access_key = \"1448875278-tFuv7p6heGXhCk5xfDQ0Btqh5sFkVkRJbodOUyI\"\n",
    "access_secret = \"SHm0LQtrwzHgpqkudPwXIJWpzZdvUAGTwBq5HSlxOzYFF\"\n",
    "\n",
    "tweetidfilename='sanders.txt'\n",
    "\n",
    "auth = OAuth(access_key, access_secret, consumer_key, consumer_secret)\n",
    "twitter = Twitter(auth = auth)\n",
    "\n",
    "tweetids=[e.strip() for e in open(tweetidfilename).readlines()]\n",
    "\n",
    "# LIMIT TO 900 TWEETS B/C OF API RESTRICTIONS AND CREATE CHUNKS OF 100\n",
    "def chunker(seq, size):\n",
    "    return [seq[pos:pos + size] for pos in range(0, len(seq), size)]\n",
    "batches = chunker(tweetids[:900],100)\n",
    "\n",
    "# API expects comma-separated strings, no lists:\n",
    "batches = [','.join(l) for l in batches]\n",
    "\n",
    "posts = []\n",
    "\n",
    "for batch in batches:\n",
    "    posts += twitter.statuses.lookup(_id=batch)\n",
    "\n",
    "\n",
    "headers=[\"text\",\"retweeted\",\"retweet_count\",\"hashtags\",\"urls\",\"created_at\",\"description\",\"screen_name\",\"name\",\"friends_count\",\"followers_count\",\"statuses_count\"]\n",
    "with open('sanders.tsv',mode='w') as fo:\n",
    "        fo.write(\"\\t\".join(headers)+\"\\n\")\n",
    "        for post in posts:\n",
    "            output=[]\n",
    "            output.append(post[\"text\"].replace(\"\\t\",\" \").replace(\"\\n\",\" \").replace(\"\\r\",\" \"))\n",
    "            output.append(str(post[\"retweeted\"]))\n",
    "            output.append(str(post[\"retweet_count\"]))            \n",
    "            hashtags = [tag[\"text\"] for tag in post[\"entities\"][\"hashtags\"]]        \n",
    "            output.append(str(hashtags))\n",
    "            urls = [u[\"expanded_url\"] for u in post[\"entities\"][\"urls\"]]    \n",
    "            output.append(str(urls))    \n",
    "            output.append(post[\"created_at\"])\n",
    "            output.append(post[\"user\"][\"description\"].replace(\"\\t\",\" \").replace(\"\\n\",\" \").replace(\"\\r\",\" \"))\n",
    "            output.append(post[\"user\"][\"screen_name\"])\n",
    "            output.append(post[\"user\"][\"name\"].replace(\"\\t\",\" \").replace(\"\\n\",\" \").replace(\"\\r\",\" \"))\n",
    "            output.append(str(post[\"user\"][\"friends_count\"]))\n",
    "            output.append(str(post[\"user\"][\"followers_count\"]))\n",
    "            output.append(str(post[\"user\"][\"statuses_count\"]))            \n",
    "            fo.write(\"\\t\".join(output)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open the file and clear the list with replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "tweetlist = []\n",
    "processedlist = []\n",
    "\n",
    "with open('/Users/shujun/Code/Python/Sentiment/Dictionary_based/Sanders.tsv')as fi:\n",
    "    reader = csv.reader(fi, delimiter='\\t')## rade the contents into a data frame\n",
    "    next(reader)##skip the header row with the next()\n",
    "    for row in reader:\n",
    "        tweet = row[0]# list\n",
    "        tweet_processed=tweet.lower().replace(\"!\",\" \").replace(\".\",\" \").replace(\"?\",\" \").replace(\"'\",\" \").replace('\"',\" \").replace(\"#\",\" \").replace(':',\" \")\n",
    "        tweetlist.append(tweet)\n",
    "        processedlist.append(tweet_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(tweetlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import positivelist and negativelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivelist = open(\"/Users/shujun/Code/Python/Sentiment/Dictionary_based/positive-words.txt\").read().splitlines()\n",
    "print(positivelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist = open(\"/Users/shujun/Code/Python/Sentiment/negative-words.txt\", encoding = \"ISO-8859-1\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment: 0.010680186833531322\n"
     ]
    }
   ],
   "source": [
    "negcountlist = []\n",
    "poscountlist = []\n",
    "sentilist = []\n",
    "\n",
    "for tweet in processedlist:\n",
    "    poscount = 0\n",
    "    negcount = 0\n",
    "#     print(\"Analyzing this one:\",tweet) #tweet is a string here\n",
    "    \n",
    "    for word in tweet.split(): ##split a string into a list\n",
    "        if word in positivelist:\n",
    "            poscount+=1\n",
    "        elif word in negativelist:\n",
    "            negcount+=1\n",
    "            \n",
    "#     print(\"It contains\", poscount, \"positive words and\", negcount, \"negative words.\")\n",
    "#     print(\"--------\")\n",
    "    \n",
    "    poscountlist.append(poscount)\n",
    "    negcountlist.append(negcount)\n",
    "    sentilist.append((poscount-negcount)/len(tweet.split()))\n",
    "print(\"Average sentiment:\",sum(sentilist)/len(sentilist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: \n",
    "next(): skip the header row\n",
    "splitlines(): splits a string into a list. The splitting is done at line breaks\n",
    "split(): splits a string into a list.You can specify the separator, default separator is any whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import vader\n",
    "senti=vader.SentimentIntensityAnalyzer()\n",
    "compoundlist = []\n",
    "for tweet in tweetlist:\n",
    "    compoundlist.append(senti.polarity_scores(tweet)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compoundlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12155276816608983\n"
     ]
    }
   ],
   "source": [
    "print(sum(compoundlist)/len(compoundlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
